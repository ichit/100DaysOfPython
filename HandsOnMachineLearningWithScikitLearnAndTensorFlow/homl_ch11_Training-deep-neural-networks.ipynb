{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 11. Training Deep Neural Networks\n",
    "\n",
    "Below are a list of the common challenges faced when training a Deep Neural Network (DNN):\n",
    "\n",
    "* *Vanishing gradients* and *exploding gradients* are when a gradient grow smaller and smaller, or larger and larger, when flowing back through the DNN. This makes training the lower levels difficult.\n",
    "* More training data is required.\n",
    "* Each iteration takes longer.\n",
    "* With more parameters to train, overfitting becomes an even bigger problem.\n",
    "\n",
    "The following sections will address (more or less) in order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "plt.style.use('seaborn-whitegrid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/daysOfCode-env/lib/python3.7/site-packages/IPython/config.py:13: ShimWarning: The `IPython.config` package has been deprecated since IPython 4.0. You should import from traitlets.config instead.\n",
      "  \"You should import from traitlets.config instead.\", ShimWarning)\n",
      "/opt/anaconda3/envs/daysOfCode-env/lib/python3.7/site-packages/ipycache.py:17: UserWarning: IPython.utils.traitlets has moved to a top-level traitlets package.\n",
      "  from IPython.utils.traitlets import Unicode\n"
     ]
    }
   ],
   "source": [
    "%load_ext ipycache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The vanishing/exploding gradients problems\n",
    "\n",
    "A *vanishing gradient* is when the gradients get smaller and smaller as the training algorithm progresses to to the lower layers.\n",
    "This results in the parameter weights in the lower layers unchanged from initialization.\n",
    "An *exploding gradient* is the opposite and the parameter weights get larger and larger and the training diverges.\n",
    "This is primary a problem in Recurrent NN, discussed in a later chapter.\n",
    "\n",
    "In 2010, Glorot and Bengio proposed that this was caused by the random initialization procedure commonly used: samples from a normal distribution.\n",
    "This caused the variance in the initial weight parameters to be greater than that of the input, thus obfuscating the important information.\n",
    "They proposed an initialization strategy, *Glorot initialization*, such that the variance of the input to the layer is the same as the variance of the output of the layer.\n",
    "There are other variants of the process that have proven to work better with various activation functions; Table 11-1 on pg. 334 shows the best initialization methods for the activation function of the neuron, and it is replecated below.\n",
    "\n",
    "| Initialization method | Activation function           | $\\sigma^2$           |\n",
    "|:----------------------|:------------------------------|:---------------------|\n",
    "| Glorot                | None, tanh, logistic, softmax | $1/fan_{\\text{avg}}$ |\n",
    "| He                    | ReLU and variants             | $2/fan_{\\text{in}}$  |\n",
    "| LeCun                 | SELU                          | $1/fan_{\\text{in}}$  |\n",
    "\n",
    "By default, Keras uses the Glorot initialization with a uniform distribution.\n",
    "The initialization can also be changed to He initialization by setting the `kernel_initializer` argument to `he_uniform` or `he_normal`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.layers.core.Dense at 0x642993c50>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras.layers.Dense(10, activation='relu', kernel_initializer='he_normal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, the details of an initializer can be specified by using the `VarianceScaling` class.\n",
    "The following example uses He initialization with a uniform distribution based on $fan_{\\text{avg}}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.layers.core.Dense at 0x6429d7e90>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "he_avg_init = keras.initializers.VarianceScaling(scale=2.0, \n",
    "                                                 mode='fan_avg', \n",
    "                                                 distribution='uniform')\n",
    "keras.layers.Dense(10, activation='sigmoid', kernel_initializer=he_avg_init)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nonsaturating activation functions\n",
    "\n",
    "In the same paper, Glorot and Bengio indicated that the activation function of choice, the logistic, was also a problem.\n",
    "Thus they proposed the *Rectified Linear Unit* (ReLU) for faster and nonsaturating training.\n",
    "The ReLU wasn't perfect, mainly because, during training, some neurons can \"die,\" meaning that they only output 0 (called a *dying ReLU*).\n",
    "Therefore, variants emerged such as the *leaky ReLU* (LeakyReLU), *Randomized leaky ReLU* (RRELU), and the *parametric leaky ReLU* (PReLU).\n",
    "The LeakyReLU includes another hyperparameter $\\alpha$ that lets the ReLU have a slightly positive slope when inputs are negative: $\\text{LeakyReLU}_\\alpha (z) = \\max(\\alpha z, z)$ (shown below).\n",
    "The RReLU is the same as the LeakyReLU, but $\\alpha$ is picked randomly within a range during training and fixed to an average during testing.\n",
    "The PReLU is where $\\alpha$ is turning into an additional parameter to be learned (this can lead to further overfitting, though)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def leakyReLU(x, alpha=0.01):\n",
    "    return [i if i>=0 else i*alpha for i in x]\n",
    "\n",
    "x = np.arange(-4, 5, 1, dtype=np.float64)\n",
    "\n",
    "plt.plot(x, np.repeat(0, len(x)), 'k-')\n",
    "plt.plot([0, 0], [-1, 5], 'k-')\n",
    "plt.plot(x, leakyReLU(x, 0.1), 'b-')\n",
    "plt.axis([-4, 4, -0.5, 4])\n",
    "plt.xlabel('$z$', fontsize=12)\n",
    "plt.ylabel('ouput', fontsize=12)\n",
    "plt.title('LeakyReLU', fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another useful activation function is the *exponential linear unit* (ELU) that is a linear funciton when the input is greater than 0 and an exponential when it is less than 0:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\text{ELU}_\\alpha (z) = \\begin{cases}\n",
    "\\alpha (\\exp(z) - 1) &\\text{ if } z < 0 \\\\\n",
    "z                    &\\text{ if } z \\ge 0\n",
    "\\end{cases}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "In addition, there is the *scaled ELU* (SELU) that will produce a network that *self-normalizes*, each layer will naturally preserve the output with a mean of 0 and standard deviation of 1.\n",
    "This is only guaranteed under the following circumstances:\n",
    "\n",
    "* The ANN must be a stack of dense, sequentially connected layers.\n",
    "* The input must be standardized to mean 0 and standard deviation 1.\n",
    "* The layer weights must be initialized with LeCun normal initialization (`kernel_initializer='lecun_normal'`).\n",
    "\n",
    "Obviously the first bullet is quite a limitation, though researchers have indicated that the SELU can be used in some cases where the layers aren't dense, such as a convolutional neuron network (Chapter 14).\n",
    "\n",
    "To use a LeakyReLU in keras, it gets added as a separate layer after the layer you want it to apply to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# An example ANN with a LeakyReLU layer.\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.InputLayer(input_shape=5),\n",
    "    keras.layers.Dense(10, kernel_initializer='he_normal'),\n",
    "    keras.layers.LeakyReLU(alpha=0.2),\n",
    "    keras.layers.Dense(1)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is an example of using the SELU activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.layers.core.Dense at 0x642a09590>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras.layers.Dense(10, activation='selu', kernel_initializer='lecun_normal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The author proposes the following order of preference for the activation functions covered above:\n",
    "\n",
    "> SELU > ELU > LeakyReLU (and its variants) > ReLU > tanh > logistic\n",
    "\n",
    "though he does provide practical reasons why you would choose one over another.\n",
    "\n",
    "### Batch normalization\n",
    "\n",
    "*Batch Normalization* (BN) is another safeguard against the vanishing and exploding gradients problems.\n",
    "BN zero-centers and normalizes each input, then scales and shifts the result using two new parameter vectors per layer: one for scaling and the other for shifting.\n",
    "The goal is to learn the optimal scale and mean for the inputs to each layer.\n",
    "To get final values for the model, a running average is maintained of each layer's mean and standard deviation.\n",
    "\n",
    "In practice, BN has shown great success in reducing the problem of vanishing and exploding gradients, even when using saturating activation functions.\n",
    "\n",
    "Though there are performance issues during training because there are now additional layers and parameters to learn, training is often slower because fewer rounds of training are required.\n",
    "Further, once the model is trained, the new layers from BN can be incorporated into the previous layer.\n",
    "This is does by updating the previous layer's weights and biases to output the correct scale and offset learned by the BN layer.\n",
    "\n",
    "Here is an example of using BN with Keras for an image classifier.\n",
    "It is added just before or after each hidden layer's activation function and as the first layer in the model (after flattening the image, though)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten (Flatten)            (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 784)               3136      \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 300)               235500    \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 300)               1200      \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 100)               30100     \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 10)                1010      \n",
      "=================================================================\n",
      "Total params: 271,346\n",
      "Trainable params: 268,978\n",
      "Non-trainable params: 2,368\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_1 = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape = [28, 28]),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dense(300, activation='elu', kernel_initializer='he_normal'),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dense(100, activation='elu', kernel_initializer='he_normal'),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dense(10, activation='softmax'),\n",
    "])\n",
    "\n",
    "model_1.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is some disagreement over whether the BN layer should be added before or after the activation functions.\n",
    "Below is an example of creating a model with the BN layer *before* the activation function.\n",
    "The activation functions must be separated from the hidden layers and added separately after the BN layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_1 (Flatten)          (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 784)               3136      \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 300)               235200    \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 300)               1200      \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 100)               30000     \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 10)                1010      \n",
      "=================================================================\n",
      "Total params: 270,946\n",
      "Trainable params: 268,578\n",
      "Non-trainable params: 2,368\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_2 = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dense(300, kernel_initializer='he_normal', use_bias=False),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Activation('elu'),\n",
    "    keras.layers.Dense(100, kernel_initializer='he_normal', use_bias=False),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Activation('elu'),\n",
    "    keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "model_2.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The \"non-trainable\" parameters the running averages for the means and standard deviations of each BN layer because they are not trained by the back-propagation.\n",
    "Below are the parameters for the first BN layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('batch_normalization/gamma:0', True),\n",
       " ('batch_normalization/beta:0', True),\n",
       " ('batch_normalization/moving_mean:0', False),\n",
       " ('batch_normalization/moving_variance:0', False)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(var.name, var.trainable) for var in model_1.layers[1].variables]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generally, the default hyperparamters for BN are good enough.\n",
    "The two that may be worth changing are `momentum` and `axis`.\n",
    "The momentum determines how much the newly computed batch means and standard deviations ($\\textbf{v}$) should contribute to the running parameter averages weights ($\\hat{\\textbf{v}}$):\n",
    "\n",
    "$\\hat{\\textbf{v}} \\leftarrow \\hat{\\textbf{v}} \\times \\text{momentum} + \\textbf{v} \\times (1 - \\text{momentum})$\n",
    "\n",
    "The axis argument determines on which axis or axes the normalization occurs.\n",
    "The default is the last axis, which for flat input data is good, but is not likely applicable for input data matrices with greater than two dimensions.\n",
    "\n",
    "### Gradient clipping\n",
    "\n",
    "The maximum and minimum gradient can be set to prevent exploding gradients.\n",
    "Two parameters in `keras.optimizers.SGD()` can be set to do this.\n",
    "The first, `clipvalue`, sets the minimum and maximum values and then just reduces any gradient over/under the values to the maximum/minimum value.\n",
    "Thus, if `clipvalue=1.0` then the maximum and minimum gradients are [-1.0, 1.0] and a gradient of [0.7, 1.1] is clipped to [0.7, 1.0].\n",
    "The main problem with setting the `clipvalue` is that is changes the *direction* of the gradient.\n",
    "This is resolved by setting `clipnorm` which scales the gradient such that all the individual values fit within the limit, thus not changing the direction, just the magnitude of the gradient.\n",
    "For example if `clipnorm=1.0` then the vector [0.9, 100.0] becomes [0.00899964, 0.9999595] instead of [0.9, 1.0] with `clipvalue=1.0`.\n",
    "\n",
    "### Using pretrained layers\n",
    "\n",
    "The author advises that, \"It is generally not a good idea to train a very large DNN from scratch,\" (HOML, pp. 345).\n",
    "Instead, a pretrained DNN can be downloaded and most of the layers reused.\n",
    "This is called *transfer learning*.\n",
    "The upsides are that is will speed up training considerably and requires less training data.\n",
    "\n",
    "In general, it is best to begin by retraining the upper layers while \"freezing\" the lower layers because they are more likely to have learned generalizable patterns.\n",
    "The more similar the new task is to the one for the original model, the more layers can be reused.\n",
    "Here is a process that will be generally advisable for *transfer learning*:\n",
    "\n",
    "1. Freeze all of the reused layers, train, and validate. This would only be training the output layer that is custom for the problem at hand.\n",
    "2. Unfreeze the top one or two layers, train, and validate. This lets back propagation train the last one or two layers in the network, leaving most of them untouched. \n",
    "3. If you have a lot of training data, you can unfreeze more layers. It is advisable to reduce the learning rate, though.\n",
    "4. Finally, try experimenting with adding additional layers or replacing the last few layers of the original model.\n",
    "\n",
    "### Transfer learning with Keras\n",
    "\n",
    "Below is an example of training `model_B` on top of `model_A`.\n",
    "First, `model_A` is created, trained, and saved to file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 38400 samples, validate on 9600 samples\n",
      "Epoch 1/3\n",
      "38400/38400 [==============================] - 11s 284us/sample - loss: 3.6792 - accuracy: 0.6599 - val_loss: 1.4006 - val_accuracy: 0.7105\n",
      "Epoch 2/3\n",
      "38400/38400 [==============================] - 10s 254us/sample - loss: 1.1440 - accuracy: 0.7265 - val_loss: 1.0446 - val_accuracy: 0.7365\n",
      "Epoch 3/3\n",
      "38400/38400 [==============================] - 10s 254us/sample - loss: 0.8715 - accuracy: 0.7540 - val_loss: 0.9083 - val_accuracy: 0.7368\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import pathlib\n",
    "\n",
    "# Pepare data.\n",
    "fashion_mnist = keras.datasets.fashion_mnist\n",
    "(X_train_full, y_train_full), (X_test, y_test) = fashion_mnist.load_data()\n",
    "\n",
    "class_names = [\"T-shirt/top\", \"Trouser\", \"Pullover\", \"Dress\", \"Coat\", \"Sandal\", \n",
    "               \"Shirt\", \"Sneaker\", \"Bag\", \"Ankle boot\"]\n",
    "\n",
    "# Split into training and validation data.\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train_full,\n",
    "                                                      y_train_full,\n",
    "                                                      test_size=0.2,\n",
    "                                                      random_state=0)\n",
    "\n",
    "# Construct model A\n",
    "model_A = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape = X_train.shape[1:]),\n",
    "    keras.layers.Dense(300, activation='elu', kernel_initializer='he_normal'),\n",
    "    keras.layers.Dense(200, activation='elu', kernel_initializer='he_normal'),\n",
    "    keras.layers.Dense(100, activation='elu', kernel_initializer='he_normal'),\n",
    "    keras.layers.Dense(50, activation='elu', kernel_initializer='he_normal'),\n",
    "    keras.layers.Dense(50, activation='elu', kernel_initializer='he_normal'),\n",
    "    keras.layers.Dense(50, activation='elu', kernel_initializer='he_normal'),\n",
    "    keras.layers.Dense(10, activation='softmax'),\n",
    "])\n",
    "\n",
    "# Compile model A\n",
    "model_A.compile(\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    optimizer=keras.optimizers.SGD(learning_rate=1e-4),\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Train model A (only a few epochs)\n",
    "history_A = model_A.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=3,\n",
    "    validation_split=0.2,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Save model to file.\n",
    "model_A_path = pathlib.Path(\"assets/ch06/ch11/model_A.h5\")\n",
    "model_A.save(model_A_path.as_posix())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the transfer learning can begin.\n",
    "First, model A is read in from file.\n",
    "Then `model_B_on_A` consists of all but the last layer of model A and a new layer is added for model B.\n",
    "The goal of model B is to classify the fashion image as shirt (positive) or sandal (negative). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get training data for just Shirts and Sandals.\n",
    "train_idx = [(y==0 or y==5) for y in y_train]\n",
    "X_train_B = X_train[train_idx, :, :]\n",
    "y_train_B = y_train[train_idx]\n",
    "y_train_B = (np.array(y_train_B) == 5)\n",
    "\n",
    "model_A = keras.models.load_model(model_A_path.as_posix())\n",
    "model_B_on_A = keras.models.Sequential(model_A.layers[:-1])\n",
    "model_B_on_A.add(keras.layers.Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, the training of `model_B_on_A` will change the layers in `model_A`.\n",
    "If this is not wanted, then model A must be cloned and the weights copied.\n",
    "An example of this is shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone the model.\n",
    "model_A_clone = keras.models.clone_model(model_A)\n",
    "\n",
    "# Copy the weights (not does by `clone_model()` method.)\n",
    "model_A_clone.set_weights(model_A.get_weights())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can train `model_B_on_A`.\n",
    "However, we want to freeze the layers from model A for the first few epochs.\n",
    "It is important to remember to recompile the model after layers are (un)frozen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7676 samples, validate on 1920 samples\n",
      "Epoch 1/4\n",
      "7676/7676 [==============================] - 3s 350us/sample - loss: 0.4199 - accuracy: 0.9491 - val_loss: 0.0441 - val_accuracy: 0.9875\n",
      "Epoch 2/4\n",
      "7676/7676 [==============================] - 2s 199us/sample - loss: 0.0622 - accuracy: 0.9875 - val_loss: 0.0360 - val_accuracy: 0.9901\n",
      "Epoch 3/4\n",
      "7676/7676 [==============================] - 2s 199us/sample - loss: 0.0554 - accuracy: 0.9889 - val_loss: 0.0336 - val_accuracy: 0.9901\n",
      "Epoch 4/4\n",
      "7676/7676 [==============================] - 2s 213us/sample - loss: 0.0520 - accuracy: 0.9902 - val_loss: 0.0308 - val_accuracy: 0.9922\n"
     ]
    }
   ],
   "source": [
    "# Freeze original layers for first training rounds.\n",
    "for layer in model_B_on_A.layers[:-1]:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Recompile the model with only the top layer to be trained.\n",
    "model_B_on_A.compile(loss='binary_crossentropy',\n",
    "                     optimizer=keras.optimizers.SGD(learning_rate=1e-3),\n",
    "                     metrics=['accuracy'])\n",
    "\n",
    "# Train the top layer.\n",
    "history = model_B_on_A.fit(X_train_B, y_train_B, epochs=4, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then the lower layers can be unfrozen, the model recompiled, and additional rounds of training run.\n",
    "Note that the learning rate is reduced by an order of magnitude."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7676 samples, validate on 1920 samples\n",
      "Epoch 1/4\n",
      "7676/7676 [==============================] - 3s 439us/sample - loss: 0.0452 - accuracy: 0.9914 - val_loss: 0.0262 - val_accuracy: 0.9927\n",
      "Epoch 2/4\n",
      "7676/7676 [==============================] - 2s 292us/sample - loss: 0.0298 - accuracy: 0.9937 - val_loss: 0.0251 - val_accuracy: 0.9927\n",
      "Epoch 3/4\n",
      "7676/7676 [==============================] - 2s 270us/sample - loss: 0.0219 - accuracy: 0.9952 - val_loss: 0.0243 - val_accuracy: 0.9927\n",
      "Epoch 4/4\n",
      "7676/7676 [==============================] - 2s 271us/sample - loss: 0.0168 - accuracy: 0.9965 - val_loss: 0.0238 - val_accuracy: 0.9932\n"
     ]
    }
   ],
   "source": [
    "for layer in model_B_on_A.layers:\n",
    "    layer.trainable = True\n",
    "\n",
    "model_B_on_A.compile(loss='binary_crossentropy',\n",
    "                     optimizer=keras.optimizers.SGD(learning_rate=1e-4),\n",
    "                     metrics=['accuracy'])\n",
    "\n",
    "history = model_B_on_A.fit(X_train_B, y_train_B, epochs=4, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    test loss: 0.029\n",
      "test accuracy: 99.25%\n"
     ]
    }
   ],
   "source": [
    "test_idx = [(y==0 or y==5) for y in y_test]\n",
    "X_test_B = X_test[test_idx, :, :]\n",
    "y_test_B = (np.array(y_test[test_idx]) == 5)\n",
    "\n",
    "test_loss, test_accuracy = model_B_on_A.evaluate(X_test_B, y_test_B, verbose=0)\n",
    "\n",
    "print(f'    test loss: {np.round(test_loss, 3)}')\n",
    "print(f'test accuracy: {np.round(test_accuracy * 100, 3)}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**In general, transfer learning is only useful for *very* deep networks, specifically convolutional neural networks (CNN, Chapter 14).**\n",
    "This is likely because shallow networks do not learn generalizable patterns in the lower layers while CNNs do.\n",
    "\n",
    "### Unsupervised pretraining\n",
    "\n",
    "If there is little labeled data and no previously trained model you can use with transfer learning, unlabeled data can be used for *unsupervised pretraining*.\n",
    "This is where an unsupervised model, often an autoencoder or generative adversarial network (GAN; Chapter 17) are trained on the unlabeled data and then the layers of the model are used as the lower layers for a DNN.\n",
    "\n",
    "### Pretraining on auxiliary data\n",
    "\n",
    "Another alternative for when there is little labeled data is to train a DNN on a related task where there is plenty of data, and then use the trained lower layers for another DNN for your desired task.\n",
    "\n",
    "## Fast optimizers\n",
    "\n",
    "Another way to reduce the amount of time required for training is to use a better optimizer than SGD.\n",
    "Below are some of the most common options.\n",
    "\n",
    "### Momentum optimization\n",
    "\n",
    "If a ball is rolling down a hill with a constant slope, it will speed up over time even though the gradient has not changed.\n",
    "This is the intuition behind *momentum optimization*.\n",
    "At each iteration, the local gradient is subtracted from the *momentum vector* $\\textbf{m}$ and updates the weights by adding the momentum vector.\n",
    "This way, the gradient acts  more as acceleration than as the speed.\n",
    "Below are the equations for calculating the momentum vector and then the parameter weights where $\\beta$ is the momentum, a parameter to limit the momentum vector, and $\\eta$ is  the learning rate, and $J(\\theta)$ is the cost function for the current value of the networks parameters $\\theta$.\n",
    "\n",
    "$\n",
    "\\textbf{m} \\leftarrow \\beta \\textbf{m} -  \\eta \\nabla_{\\theta} J(\\theta) \\\\\n",
    "\\theta \\leftarrow \\theta + \\textbf{m}\n",
    "$\n",
    "\n",
    "To use the momentum optimization in keras, set the `momentum` parameter in the the `SGD` optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.optimizer_v2.gradient_descent.SGD at 0x1a54af4cd0>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras.optimizers.SGD(learning_rate=1e-3, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nesterov accelerated gradient\n",
    "\n",
    "The *Nesterov accelerated gradient* (NAG) is a slight enhancement of momentum optimization.\n",
    "It measures the gradient not at the current location in the parameter space, but slightly ahead at $\\theta + \\beta \\textbf{m}$:\n",
    "\n",
    "$\n",
    "\\textbf{m} \\leftarrow \\beta \\textbf{m} -  \\eta \\nabla_{\\theta} J(\\theta + \\beta \\textbf{m}) \\\\\n",
    "\\theta \\leftarrow \\theta + \\textbf{m}\n",
    "$\n",
    "\n",
    "It generally helps by slightly adjusting the current direction of the momentum vector towards the minima.\n",
    "It is trivial to implement in keras, and because it is usually faster than momentum optimization, it is recommended to apply:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.optimizer_v2.gradient_descent.SGD at 0x1a5548ef90>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras.optimizers.SGD(learning_rate=1e-3, momentum=0.9, nesterov=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AdaGrad\n",
    "\n",
    "AdaGrad uses an *adaptive learning rate* strategy to scale (decay) the learning rate faster for steeper dimensions.\n",
    "It is very simillar to SGD, but divides the loss for each parameter by the square of the gradient (with an additional smoothing value $\\epsilon$):\n",
    "\n",
    "$\n",
    "\\textbf{s} \\leftarrow \\textbf{s} + \\nabla_\\theta J(\\theta) \\otimes \\nabla_\\theta J(\\theta)  \\\\\n",
    "\\theta \\leftarrow \\theta - \\eta \\nabla_\\theta J(\\theta) \\oslash \\sqrt{\\textbf{s} + \\epsilon}\n",
    "$\n",
    "\n",
    "**AdaGrad scales the learning rate too quickly for training DNNs - do NOT use it.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.optimizer_v2.adagrad.Adagrad at 0x1a551ca1d0>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras.optimizers.Adagrad(learning_rate=1e-3, epsilon=1e-7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(The default behaviour of many optimizers is to use `keras.backend.epsilon()` of `epsilon`.\n",
    "It is effectively a global variable that can be set using `keras.backend.set_epsilon()`.\n",
    "If the default value for `epsilon` is `None`, then it is likely using this value.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RMSProp\n",
    "\n",
    "This is simillar to AdaGrad, but fixes the issue of decaying the gradient too quickly by accumulating only the more recent gradients, where $\\beta$ is  the decay rate (the memory):\n",
    "\n",
    "$\n",
    "\\textbf{s} \\leftarrow \\beta \\textbf{s} + (1 - \\beta) \\nabla_\\theta J(\\theta) \\otimes \\nabla_\\theta J(\\theta)  \\\\\n",
    "\\theta \\leftarrow \\theta - \\eta \\nabla_\\theta J(\\theta) \\oslash \\sqrt{\\textbf{s} + \\epsilon}\n",
    "$\n",
    "\n",
    "$\\beta$ is another hyperparameter to tune, though the default value of 0.9 is usually sufficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.optimizer_v2.rmsprop.RMSprop at 0x1a5563c6d0>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras.optimizers.RMSprop(learning_rate=1e-3, rho=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adam and  Nadam  Optimization\n",
    "\n",
    "The *adaptive moment estimation* (Adam) optimizer uses a combination of momentum optimization and RMSProp (the simillarities are quite obvious in the algorithm on pp. 356 of *HOML*.\n",
    "There are three hyperparameters to set, though the defaults (used below) are usually sufficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.optimizer_v2.adam.Adam at 0x1a54ad1950>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras.optimizers.Adam(learning_rate=1e-3, \n",
    "                      beta_1=0.9, \n",
    "                      beta_2=0.999, \n",
    "                      epsilon=1e-7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two optimizers derived from Adam that the author touched upon.\n",
    "*AdaMax* is a reworking of the Adam algorithm to make it a bit more stable.\n",
    "Still, Adam tends to perform better.\n",
    "*Nadam* is Adam with the Nesterov trick.\n",
    "It usually converges faster and is generally recommended to try it out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.optimizer_v2.nadam.Nadam at 0x1a552449d0>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# AdaMax optimizer (default arguments)\n",
    "keras.optimizers.Adamax(learning_rate=0.001,\n",
    "                        beta_1=0.9,\n",
    "                        beta_2=0.999,\n",
    "                        epsilon=1e-07)\n",
    "\n",
    "# Nadam optimizer (default arguments)\n",
    "keras.optimizers.Nadam(learning_rate=0.001,\n",
    "                       beta_1=0.9,\n",
    "                       beta_2=0.999,\n",
    "                       epsilon=1e-07)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Rate Scheduling\n",
    "\n",
    "A dynamic learning rate, one that changes during the training, is usually better than a static learning rate.\n",
    "Here are some of the more popular *learning schedules*.\n",
    "\n",
    "* **Power scheduling**: The learning rate is a function of the iteration number $t$: $\\eta(t) = \\eta_0 / (1 + t/s)^c$ where $s$, $c$, and $\\eta_0$ are all hyperparameters to tune. This equation drops the learning rate quickly at first, but then the changes are smaller.\n",
    "* **Exponential scheduling**: Set the learning rate to $\\eta(t) = \\eta_0 0.1^{t/s}$. This equation reduces the learning rate by a factor of 10 every $s$ steps.\n",
    "* **Piecewise constant scheduling**: Assign specific learning rates to each range of training iterations.\n",
    "* **Performance scheduling**: Measure the validation error every $N$ steps and reduce the learning rate by a factor of  $\\gamma$ when the error stops dropping.\n",
    "* **Icycle scheduling**: This is a more complicated schedule that raises the learning rate linearly during the first half of training, and then reduces it linearly in the second half. It does the opposite for the momentum.\n",
    "\n",
    "Implementing these in keras is pretty easy.\n",
    "First, power  scheduling can be accomplished using the `decay` argument in the `SGD` optimizer class. (The $c$ hyperparameter is automatically set to 1.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.optimizer_v2.gradient_descent.SGD at 0x64246eb50>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras.optimizers.SGD(learning_rate=1e-2, momentum=0.9, decay=1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementing exponential scheduling is fairly straightforward, too.\n",
    "First, a function that takes the iteration number and returns the learning rate must be defined.\n",
    "Then, a `LearningRateScheduler` callback is made an provided the function.\n",
    "The callback is then passed to the `fit()` method.\n",
    "\n",
    "Here, the function `exponential_decay()` takes the initial learning rate and $s$ hyperparameter value and returns another function that implements the exponential decay scheduler for a given epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exponential_decay(lr0, s):\n",
    "    \"\"\"Return a LR scheduling function with the desired hyperparameters.\"\"\"\n",
    "    return lambda epoch: lr0 * 0.1**(epoch / s)\n",
    "\n",
    "# Learning rate scheduling function with initial learning rate and s.\n",
    "exponential_decay_fn = exponential_decay(lr0=0.01, s=20)\n",
    "\n",
    "# A learning rate scheduler callback to implement the exponential scheduler.\n",
    "lr_scheduler = keras.callbacks.LearningRateScheduler(exponential_decay_fn)\n",
    "\n",
    "# Pass `lr_scheduler` to the `fit()` method for training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If needed, the schedule function can take the current learning rate as a second argument.\n",
    "\n",
    "The *performance scheduler* can be implemented using the `ReduceLROnPlateau` callback.\n",
    "It multiplies the current learning rate by a desired factor after a certain number of epochs without improvement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.ReduceLROnPlateau at 0x1a5547a0d0>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras.callbacks.ReduceLROnPlateau(factor=0.5, patience=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorFlow's implementation of Keras offers the ability to define a learning rate schedule using an object in `keras.optimizers.schedules` and then pass it to an optimizer.\n",
    "**This approach updates the learing rate at each step rather than at each epoch.**\n",
    "Below is an example of exponential decay where the number of steps is the total number of training steps in 20 epochs and a batch size of 32."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training hyperparameters.\n",
    "n_epochs = 20\n",
    "batch_size = 32\n",
    "\n",
    "# Learning rate scheduler.\n",
    "s = n_epochs * len(X_train) // batch_size\n",
    "learning_rate = keras.optimizers.schedules.ExponentialDecay(0.01, s, 0.1)\n",
    "\n",
    "# SGD using the exponential LR scheduler.\n",
    "optimizer =  keras.optimizers.SGD(learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Avoiding overfitting through regularization\n",
    "\n",
    "We have already covered two powerful techniques that act like regularization: early stopping and Batch Normalization.\n",
    "\n",
    "### $\\ell_1$ and $\\ell_2$ regularization\n",
    "\n",
    "Both $\\ell_1$ and $\\ell_2$ can easily be used to constrain a layer's weights. \n",
    "$\\ell_1$ forces more weights to 0 and thus creates more sparse networks.\n",
    "Here is an example of a dense layer that applies $\\ell_2$ regularization to its connection weights with a regularization factor of 0.01 (how strong of a penalty to use):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.layers.core.Dense at 0x6430f5dd0>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras.layers.Dense(100,\n",
    "                   activation='elu',\n",
    "                   kernel_initializer='he_normal',\n",
    "                   kernel_regularizer=keras.regularizers.l2(0.01))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, defining this for each layer of a network would become repetitive.\n",
    "One way around this would be to implement a for-loop.\n",
    "Alternatively, Python's `functools.partial()`, a function that can create a thin wrapper for any callable, can be used to create a proto-layer with desireable defaults."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "# A wrapper around a Dense layer with L2-regularization.\n",
    "RegularizedDense = partial(\n",
    "    keras.layers.Dense,\n",
    "    activation='elu',\n",
    "    kernel_initializer='he_normal',\n",
    "    kernel_regularizer=keras.regularizers.l2(0.01)\n",
    ")\n",
    "\n",
    "# A sequential model.\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    RegularizedDense(300),\n",
    "    RegularizedDense(100),\n",
    "    RegularizedDense(10, \n",
    "                     activation='softmax', \n",
    "                     kernel_initializer='glorot_uniform'),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropout\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
