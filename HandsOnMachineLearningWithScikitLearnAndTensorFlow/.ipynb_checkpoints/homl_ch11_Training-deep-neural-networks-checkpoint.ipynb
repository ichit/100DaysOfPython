{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 11. Training Deep Neural Networks\n",
    "\n",
    "Below are a list of the common challenges faced when training a Deep Neural Network (DNN):\n",
    "\n",
    "* *Vanishing gradients* and *exploding gradients* are when a gradient grow smaller and smaller, or larger and larger, when flowing back through the DNN. This makes training the lower levels difficult.\n",
    "* More training data is required.\n",
    "* Each iteration takes longer.\n",
    "* With more parameters to train, overfitting becomes an even bigger problem.\n",
    "\n",
    "The following sections will address (more or less) in order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "plt.style.use('seaborn-whitegrid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The ipycache extension is already loaded. To reload it, use:\n",
      "  %reload_ext ipycache\n"
     ]
    }
   ],
   "source": [
    "%load_ext ipycache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The vanishing/exploding gradients problems\n",
    "\n",
    "A *vanishing gradient* is when the gradients get smaller and smaller as the training algorithm progresses to to the lower layers.\n",
    "This results in the parameter weights in the lower layers unchanged from initialization.\n",
    "An *exploding gradient* is the opposite and the parameter weights get larger and larger and the training diverges.\n",
    "This is primary a problem in Recurrent NN, discussed in a later chapter.\n",
    "\n",
    "In 2010, Glorot and Bengio proposed that this was caused by the random initialization procedure commonly used: samples from a normal distribution.\n",
    "This caused the variance in the initial weight parameters to be greater than that of the input, thus obfuscating the important information.\n",
    "They proposed an initialization strategy, *Glorot initialization*, such that the variance of the input to the layer is the same as the variance of the output of the layer.\n",
    "There are other variants of the process that have proven to work better with various activation functions; Table 11-1 on pg. 334 shows the best initialization methods for the activation function of the neuron, and it is replecated below.\n",
    "\n",
    "| Initialization method | Activation function           | $\\sigma^2$           |\n",
    "|:----------------------|:------------------------------|:---------------------|\n",
    "| Glorot                | None, tanh, logistic, softmax | $1/fan_{\\text{avg}}$ |\n",
    "| He                    | ReLU and variants             | $2/fan_{\\text{in}}$  |\n",
    "| LeCun                 | SELU                          | $1/fan_{\\text{in}}$  |\n",
    "\n",
    "By default, Keras uses the Glorot initialization with a uniform distribution.\n",
    "The initialization can also be changed to He initialization by setting the `kernel_initializer` argument to `he_uniform` or `he_normal`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.layers.core.Dense at 0x6404ed310>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras.layers.Dense(10, activation='relu', kernel_initializer='he_normal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, the details of an initializer can be specified by using the `VarianceScaling` class.\n",
    "The following example uses He initialization with a uniform distribution based on $fan_{\\text{avg}}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.layers.core.Dense at 0x114fda490>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "he_avg_init = keras.initializers.VarianceScaling(scale=2.0, \n",
    "                                                 mode='fan_avg', \n",
    "                                                 distribution='uniform')\n",
    "keras.layers.Dense(10, activation='sigmoid', kernel_initializer=he_avg_init)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nonsaturating activation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
