# Processing Sequences Using RNNs and CNNs

Recurrent neural networks can work on sequences of arbitrary length, making them very useful for time series data or text processing.


```python
import numpy as np
import pandas as pd 
import matplotlib as mpl
import matplotlib.pyplot as plt
import seaborn as sns
import tensorflow as tf 
import tensorflow.keras as keras

%matplotlib inline
np.random.seed(0)
sns.set_style('whitegrid')
```

## Recurrent neurons and layers

A recurrent neuron looks just like a normal feedforward neuron except it also has connections pointing backwards.
At each *time step t* (or *frame*), a recurrent neuron receives the inputs $\textbf{x}_{(t)}$ as well as its own output from a previous time step, $\textbf{y}_{(t-1)}$.
Thus, each neuron has two sets of weights, $\textbf{w}_x$ and $\textbf{w}_y$.
These inputs and weights get multiplied together and passed to an activation function just like for a feedforward network.
The following function is for a layer of recurrent neurons at a time frame $t$ where $\phi$ is the activation function and $b$ is the bias.

$$
\textbf{y}_{(t)} = \phi(\textbf{W}_x^T \textbf{x}_{(t)} + \textbf{W}_y^T \textbf{y}_{(t-1)} + b)
$$

Generally, the initial value of $\textbf{y}$ at $t=0$ is set to 0.
It is common to see a recurrent neuron displayed across the time axis - this is called *unrolling the network through time*.

### Memory cells

A recurrent neuron has a form of memory because its output is a function of the data at the current and previous time step.
A part of a neural network that preserves some state acrosss time steps is called a *memory cell* (or just a *cell*).

### Input and output sequences

There are various types of RNNs based on their input and output types:

* *sequence-to-sequences*: An RNN that takes a sequence of inputs and produces a sequence of outputs.
* *sequence-to-vector*: A network that takes a sequence and only the final output is used.
* *vector-to-sequence*: The same input vector is passed in repetitively and an output sequence is collected.
* *encoder-decoder*: A sequence-to-vector network (encoder) followed by a vector-to-sequence network (decoder).

## Training RNNs

The trick to training an RNN is to unroll the network through time and use regular backpropagation.
This is called *backpropagation through time* (BPTT).
Thankfully, this hassle is handled by TF Keras.

## Forecasting a time series

A time series can be *univariate* or *multivariate* with a single or multiple feature at each time step, respectively.
Time series analyses are often either forecasting, predicting future values, or imputing, filling in missing data.

For simplicity, we will use a mock time series data set generated by the `generate_time_series()` function.
It returns data of the shape $[batch\ size,\ time\ steps,\ dimensionality]$.
The *dimensionality* refers to if the data is univariate or multivariate.


```python
def generate_time_series(batch_size, n_steps):
    freq1, freq2, offsets1, offsets2 = np.random.rand(4, batch_size, 1)
    time = np.linspace(0, 1, n_steps)
    series = 0.5 * np.sin((time-offsets1) * (freq1 * 10 + 10))   # wave 1
    series += 0.2 * np.sin((time-offsets2) * (freq2 * 20 + 20))  # wave 2
    series += 0.1 * (np.random.rand(batch_size, n_steps) - 0.5)  # noise
    return series[..., np.newaxis].astype(np.float32)
```


```python
np.random.seed(0)
x = generate_time_series(3, 100)

fig = plt.figure(figsize=(8, 5))
plt.plot(x[0, :], 'g-')
plt.plot(x[1, :], 'b-')
plt.plot(x[2, :], 'r-')
plt.xlabel('time', fontsize=14)
plt.ylabel('value', fontsize=14)
plt.title('Mock time series', fontsize=18)
plt.show()
```


![png](homl_ch15_Processing-sequences-using-RNNs-and-CNNs_files/homl_ch15_Processing-sequences-using-RNNs-and-CNNs_4_0.png)


Now we can create some mock data and split it into training, validation, and test sets.
This mock data will have 10,000 times series, each 50 steps long.


```python
n_steps = 50
series = generate_time_series(10000, n_steps + 1)
X_train, y_train = series[:7000, :n_steps], series[:7000, -1]
X_valid, y_valid = series[7000:9000, :n_steps], series[7000:9000, -1]
X_test, y_test = series[9000:, :n_steps], series[9000:, -1]
```

### Baseline metrics

A baseline metric will help us recognize if our model is actually worth creating.
It provides some indication of how hard the data is to predict.

A simple metric called *naive forecasting* is to simply predict the previous value.
In this case, is has a MSE of 0.021.


```python
naive_pred = X_valid[:, -1]
np.mean(keras.losses.mean_squared_error(y_valid, naive_pred))
```




    0.021185566



Another approach is to use a very simple neural network.
Here we can use a single dense neuron; the output is thus a linear combination of the inputs.
This model can make predictions on the validation data with a MSE of 0.004.


```python
model = keras.models.Sequential([
    keras.layers.Flatten(input_shape=[50, 1]),
    keras.layers.Dense(1)
])

model.compile(
    optimizer=keras.optimizers.Nadam(),
    loss=keras.losses.MeanSquaredError()
)

history = model.fit(
    X_train, y_train,
    epochs=20,
    validation_data=(X_valid, y_valid),
    verbose=0
)
```


```python
pd.DataFrame(history.history).plot(figsize=(8, 6))
plt.show()
```


![png](homl_ch15_Processing-sequences-using-RNNs-and-CNNs_files/homl_ch15_Processing-sequences-using-RNNs-and-CNNs_11_0.png)



```python
y_pred = model.predict(X_valid)
np.mean(keras.losses.mean_squared_error(y_valid, y_pred))
```




    0.0037825033



### Implementing a simple RNN

The simplest RNN we can create is just a single neuron.
Thus, there are a total of 3 parameters: a weight for the input, a weight for the previous output, and a bias term.
This is far less than the 51 parameters for the linear model above (1 parameter per input feature and a bias term).
Thus, it is not surprising to see that this simple model cannot outperform a higher-dimensional linear model.

Note that the input shape is `[None, 1]`.
This is because the RNN can take any length of input (number of time steps).


```python
simple_rnn = keras.models.Sequential([
    keras.layers.SimpleRNN(1, input_shape=[None, 1])
])

simple_rnn.compile(
    optimizer=keras.optimizers.Nadam(),
    loss=keras.losses.MeanSquaredError()
)

history = simple_rnn.fit(
    X_train, y_train,
    epochs=10,
    validation_data=(X_valid, y_valid),
    verbose=0
)
```


```python
pd.DataFrame(history.history).plot(figsize=(8, 6))
plt.show()
```


![png](homl_ch15_Processing-sequences-using-RNNs-and-CNNs_files/homl_ch15_Processing-sequences-using-RNNs-and-CNNs_15_0.png)



```python
y_pred = simple_rnn.predict(X_valid)
np.mean(keras.losses.mean_squared_error(y_valid, y_pred))
```




    0.1480416



### Deep RNNs

Keras makes it easy to make a deep RNN by just stacking `SimpleRNN` layers.
Note that it is necessary to explicitly tell each layer to output the entire sequence so that the next layer doesn't just receive the final output of the previous one.

We can use another `SimpleRNN` for the final layer, but it is often preferrable to use a single `Dense` neuron.
One reason is that there is only one hidden variable for the single recurrent neuron whereas the normal neuron has a weight for each input.
Also, the recurrent neuron uses the tanh activation function, limiting the output to between -1 and 1.


```python
deep_rnn = keras.models.Sequential([
    keras.layers.SimpleRNN(20, return_sequences=True, input_shape=[None, 1]),
    keras.layers.SimpleRNN(20, return_sequences=False),
    keras.layers.Dense(1)
])

deep_rnn.compile(
    optimizer=keras.optimizers.Nadam(),
    loss=keras.losses.MeanSquaredError()
)

history = deep_rnn.fit(
    X_train, y_train,
    epochs=10,
    validation_data=(X_valid, y_valid),
    verbose=0
)
```


```python
pd.DataFrame(history.history).plot(figsize=(8, 6))
plt.show()
```


![png](homl_ch15_Processing-sequences-using-RNNs-and-CNNs_files/homl_ch15_Processing-sequences-using-RNNs-and-CNNs_19_0.png)



```python
y_pred = deep_rnn.predict(X_valid)
np.mean(keras.losses.mean_squared_error(y_valid, y_pred))
```




    0.002750152



### Forecasting several time steps ahead

So far, we have only predicted the value at the next time step, but we could have predicted the value after 10 additional steps by changing how we split the mock data into X and y datasets.
But what if we wanted to predict all of the next 10 steps.

One way to do this is to use the model trained above to predict the next step, then add that output to the input and have it predict the next step after that, and so on.


```python
np.random.seed(23)
series = generate_time_series(1, n_steps + 10)
X_new, Y_new = series[:, :n_steps], series[:, n_steps:]
X = X_new
for step_ahead in range(10):
    y_pred_one = model.predict(X[:, step_ahead:])[:, np.newaxis, :]
    X = np.concatenate([X, y_pred_one], axis=1)

Y_pred = X[:, n_steps:]
```


```python
fig = plt.figure(figsize=(8, 5))
plt.plot(range(X_new.shape[1]), 
         X_new[0, :, 0], 
         'k-o')
plt.plot(range(X_new.shape[1], X_new.shape[1] + Y_new.shape[1]), 
         Y_new[0, :, 0], 
         'g--x', label='real')
plt.plot(range(X_new.shape[1], X_new.shape[1] + Y_pred.shape[1]), 
         Y_pred[0, :, 0], 
         'r--o', label='forecast')
plt.xlabel('time step', fontsize=14)
plt.ylabel('value', fontsize=14)
plt.title('Forecasting several times steps ahead', fontsize=18)
plt.legend(loc='best')
plt.show()
```


![png](homl_ch15_Processing-sequences-using-RNNs-and-CNNs_files/homl_ch15_Processing-sequences-using-RNNs-and-CNNs_23_0.png)


Another option is to train the RNN to predict all 10 next values at once.
We will still use a sequence to vector model, but there will be 10 output values instead of 1.
We must first, however, change the targets to be vectors containing the last 10 values.


```python
np.random.seed(0)
series = generate_time_series(10000, n_steps + 10)
X_train, Y_train = series[:7000, :n_steps], series[:7000, -10:, 0]
X_valid, Y_valid = series[7000:9000, :n_steps], series[7000:9000, -10:, 0]
X_test, Y_test = series[9000:, :n_steps], series[9000:, -10:, 0]
```

The only change to the model is that the final dense layer must have 10 neurons.


```python
deep_rnn_10steps = keras.models.Sequential([
    keras.layers.SimpleRNN(20, return_sequences=True, input_shape=[None, 1]),
    keras.layers.SimpleRNN(20),
    keras.layers.Dense(10)
])

deep_rnn_10steps.compile(
    optimizer=keras.optimizers.Nadam(),
    loss=keras.losses.MeanSquaredError()
)

history = deep_rnn_10steps.fit(
    X_train, Y_train,
    epochs=10,
    validation_data=(X_valid, Y_valid),
    verbose=0
)
```


```python
pd.DataFrame(history.history).plot(figsize=(8, 6))
plt.show()
```


![png](homl_ch15_Processing-sequences-using-RNNs-and-CNNs_files/homl_ch15_Processing-sequences-using-RNNs-and-CNNs_28_0.png)



```python
Y_pred = deep_rnn_10steps.predict(X_valid)
np.mean(keras.losses.mean_squared_error(Y_valid, Y_pred))
```




    0.010641455




```python
Y_pred = deep_rnn_10steps.predict(X_new)

fig = plt.figure(figsize=(8, 5))
plt.plot(range(X_new.shape[1]), 
         X_new[0, :, 0], 
         'k-o')
plt.plot(range(X_new.shape[1], X_new.shape[1] + Y_new.shape[1]), 
         Y_new[0, :, 0], 
         'g--x', label='real')
plt.plot(range(X_new.shape[1], X_new.shape[1] + Y_pred.shape[1]), 
         Y_pred[0, :], 
         'r--o', label='forecast')
plt.xlabel('time step', fontsize=14)
plt.ylabel('value', fontsize=14)
plt.title('Forecasting several times steps ahead', fontsize=18)
plt.legend(loc='best')
plt.show()
```


![png](homl_ch15_Processing-sequences-using-RNNs-and-CNNs_files/homl_ch15_Processing-sequences-using-RNNs-and-CNNs_30_0.png)


We can do still better by having the model constantly trying to predict the next 10 steps from the very beginning, not just the very end.
This basically increases the amount of training data for the model.

To turn the model into a sequence-to-sequence model, `return_sequence` must be set to `True` for every recurrent layer.
Also, the final dense output must be applied at every time step.
For this, Keras offers the `TimeDistributed` layer.
It wraps another layer and applied it at every time step of its input sequence.
It treats each time step as a new input, thus reshaping the input from $[batch\ size,\ time\ steps,\ input\ dimension]$ to $[batch\ size\ \times\ time\ steps,\ input\ dimension]$

First, we must change the target data.
Each target must be a sequence of 10-dimensional vectors of the same length as the input sequence.


```python
Y = np.empty((10000, n_steps, 10))
for step_ahead in range(1, 10 + 1):
    Y[:, :, step_ahead - 1] = series[:, step_ahead:step_ahead + n_steps, 0]
Y_train = Y[:7000]
Y_valid = Y[7000:9000]
Y_test = Y[9000:]
```


```python
Y_train.shape
```




    (7000, 50, 10)



All outputs from the model are used for training, but we only care about the last time step for prediction and evaluation purposes.
Thus, we can use a custom metric to compute the MSE over the output at the last time step.


```python
def last_time_step_mse(Y_true, Y_pred):
    return keras.metrics.mean_squared_error(Y_true[:, -1], Y_pred[:, -1])
```


```python
s2s_rnn = keras.models.Sequential([
    keras.layers.SimpleRNN(20, return_sequences=True, input_shape=[None, 1]),
    keras.layers.SimpleRNN(20, return_sequences=True),
    keras.layers.TimeDistributed(keras.layers.Dense(10))
])

s2s_rnn.compile(
    optimizer=keras.optimizers.Nadam(),
    loss=keras.losses.MeanSquaredError(),
    metrics=[last_time_step_mse]
)

history = s2s_rnn.fit(
    X_train, Y_train,
    epochs=13,
    validation_data=(X_valid, Y_valid),
    verbose=0
)
```


```python
pd.DataFrame(history.history).plot(figsize=(8, 6))
plt.show()
```


![png](homl_ch15_Processing-sequences-using-RNNs-and-CNNs_files/homl_ch15_Processing-sequences-using-RNNs-and-CNNs_37_0.png)



```python
Y_pred = s2s_rnn.predict(X_valid)
np.mean(keras.losses.mean_squared_error(Y_valid, Y_pred))
```




    0.028833391




```python
Y_pred = s2s_rnn.predict(X_new)

fig = plt.figure(figsize=(8, 5))
plt.plot(range(X.shape[1]), X[0, :, 0], 'k-')

for i in range(1, Y_pred.shape[2] + 1):
    plt.plot(range(i, X_new.shape[1] + i),
             Y_pred[0, :, i-1],
             'r--', label=i, alpha=0.5)

plt.xlabel('time step', fontsize=14)
plt.ylabel('value', fontsize=14)
plt.title('Forecasting several times steps ahead', fontsize=18)
plt.show()
```


![png](homl_ch15_Processing-sequences-using-RNNs-and-CNNs_files/homl_ch15_Processing-sequences-using-RNNs-and-CNNs_39_0.png)


## Handling long sequences

A nonsaturating activation function (e.g. ReLU) can leading to exploding outputs.
Thus a saturating activation function, such as the hyperbolic tangent, is often the best choice (and is the default in Keras).

Batch Normalization is not tremendously useful in RNNs, but instead *Layer Normalization* (LN) has proven successful.
Instead of normalizing across the batch dimension, it normalizes across the features dimension.
It has a few advantages including the ability to compute the required statistics on the fly at each time step, independently for each instance; it does not compute a running average like BN.
LN learns a scale and offset parameter for each input and is typically used right after the linear combination of inputs and the hidden states.

To implement LN in a single memory cell, we must make a custom memory cell.
It is similar to a normal layer except its `call()` method takes two inputs: the `inputs` at the current time step, and the hidden `states` from the previous time step.
Also, the cell must have a `state_size` and an `output_size` attributes, which for a simple RNN, are equal to the number of units.

The following `LNSimpleRNNCell` is a custom memory cell that will behave just like a `SimpleRNNCell` except it will employ Layer Normalization.


```python
class LNSimpleRNNCell(keras.layers.Layer):
    def __init__(self, units, activation='tanh', **kwargs):
        super().__init__(**kwargs)
        self.state_size = units
        self.output_size = units
        self.simple_rnn_cell = keras.layers.SimpleRNNCell(units,
                                                          activation=None)
        self.layer_norm = keras.layers.LayerNormalization()
        self.activation = keras.activations.get(activation)

    def call(self, inputs, states):
        outputs, new_stats = self.simple_rnn_cell(inputs, states)
        norm_outputs = self.activation(self.layer_norm(outputs))
        return norm_outputs, [norm_outputs]
```

Now the new cell can be used in a RNN by passing it to the `RNN` layer in Keras.


```python
ln_rnn = keras.models.Sequential([
    keras.layers.RNN(LNSimpleRNNCell(20), 
                     return_sequences=True, 
                     input_shape=[None, 1]),
    keras.layers.RNN(LNSimpleRNNCell(20), return_sequences=True),
    keras.layers.TimeDistributed(keras.layers.Dense(10))
])

ln_rnn.compile(
    optimizer=keras.optimizers.Nadam(),
    loss=keras.losses.MeanSquaredError(),
    metrics=[last_time_step_mse]
)

history = ln_rnn.fit(
    X_train, Y_train,
    epochs=10,
    validation_data=(X_valid, Y_valid),
    verbose=0
)
```


```python
pd.DataFrame(history.history).plot(figsize=(8, 6))
plt.show()
```


![png](homl_ch15_Processing-sequences-using-RNNs-and-CNNs_files/homl_ch15_Processing-sequences-using-RNNs-and-CNNs_44_0.png)



```python
Y_pred = ln_rnn.predict(X_valid)
np.mean(keras.losses.mean_squared_error(Y_valid, Y_pred))
```




    0.036504794




```python
Y_pred = ln_rnn.predict(X_new)

fig = plt.figure(figsize=(8, 5))
plt.plot(range(X.shape[1]), X[0, :, 0], 'k-')

for i in range(1, Y_pred.shape[2] + 1):
    plt.plot(range(i, X_new.shape[1] + i),
             Y_pred[0, :, i-1],
             'r--', label=i, alpha=0.5)

plt.xlabel('time step', fontsize=14)
plt.ylabel('value', fontsize=14)
plt.title('Forecasting several times steps ahead', fontsize=18)
plt.show()
```


![png](homl_ch15_Processing-sequences-using-RNNs-and-CNNs_files/homl_ch15_Processing-sequences-using-RNNs-and-CNNs_46_0.png)


### Tackling the short-term memory problem

Some information is lost at each time-step of an RNN to the point where by the end of the sequence, there is no trace of the initial inputs.
Various types of cells with long-term memory have been created to tackle this problem.

#### Long Short-Term Memory (LSTM) cells

The *Long Short-Term Memory* (LSTM) cell was created in 1997 and steadily improved since.
With Keras, it can be used as a drop-in replacement for the `SimpleRNN` layers we have been using thus far.
This will lead to better performance and faster training as well as retaining long=term dependencies in the data.


```python
lstm_rnn = keras.models.Sequential([
    keras.layers.LSTM(20, return_sequences=True, input_shape=[None, 1]),
    keras.layers.LSTM(20, return_sequences=True),
    keras.layers.TimeDistributed(keras.layers.Dense(10))
])

lstm_rnn.compile(
    optimizer=keras.optimizers.Nadam(),
    loss=keras.losses.MeanSquaredError(),
    metrics=[last_time_step_mse]
)

history = lstm_rnn.fit(
    X_train, Y_train,
    epochs=10,
    validation_data=(X_valid, Y_valid),
    verbose=0
)
```


```python
pd.DataFrame(history.history).plot(figsize=(8, 6))
plt.show()
```


![png](homl_ch15_Processing-sequences-using-RNNs-and-CNNs_files/homl_ch15_Processing-sequences-using-RNNs-and-CNNs_49_0.png)



```python
Y_pred = lstm_rnn.predict(X_valid)
np.mean(keras.losses.mean_squared_error(Y_valid, Y_pred))
```




    0.02758335




```python
Y_pred = lstm_rnn.predict(X_new)

fig = plt.figure(figsize=(8, 5))
plt.plot(range(X.shape[1]), X[0, :, 0], 'k-')

for i in range(1, Y_pred.shape[2] + 1):
    plt.plot(range(i, X_new.shape[1] + i),
             Y_pred[0, :, i-1],
             'r--', label=i, alpha=0.5)

plt.xlabel('time step', fontsize=14)
plt.ylabel('value', fontsize=14)
plt.title('Forecasting several times steps ahead', fontsize=18)
plt.show()
```


![png](homl_ch15_Processing-sequences-using-RNNs-and-CNNs_files/homl_ch15_Processing-sequences-using-RNNs-and-CNNs_51_0.png)


The author continued on to explain LSTM in more detail, followed by explaining some of the popular variants including those with *peephole connections* and *Gated Recurrent Unit* (GRU) cells.
He also provides explanations for implementing them both in Keras.

#### Using 1D convolutional layers to process sequences

It is also possible to use a convolutional layer to reduce the length of the sequence.
Further, the filters are able to learn more subtle short-term patterns.
Below is an example where the first layer is now a convolutional layer.
Notice that the model learns much faster and performs better, too.


```python
conv_rnn = keras.models.Sequential([
    keras.layers.Conv1D(filters=20, kernel_size=4, strides=2, padding="valid", 
                        input_shape=[None, 1]),
    keras.layers.GRU(20, return_sequences=True),
    keras.layers.GRU(20, return_sequences=True),
    keras.layers.TimeDistributed(keras.layers.Dense(10))
])

conv_rnn.compile(
    optimizer=keras.optimizers.Nadam(),
    loss=keras.losses.MeanSquaredError(),
    metrics=[last_time_step_mse]
)

history = conv_rnn.fit(
    X_train, Y_train[:, 3::2],
    epochs=10,
    validation_data=(X_valid, Y_valid[:, 3::2]),
    verbose=0
)
```


```python
pd.DataFrame(history.history).plot(figsize=(8, 6))
plt.show()
```


![png](homl_ch15_Processing-sequences-using-RNNs-and-CNNs_files/homl_ch15_Processing-sequences-using-RNNs-and-CNNs_54_0.png)



```python
Y_pred = conv_rnn.predict(X_valid)
np.mean(keras.losses.mean_squared_error(Y_valid[:, 3::2], Y_pred))
```




    0.021186305




```python
Y_pred = conv_rnn.predict(X_new)

fig = plt.figure(figsize=(8, 5))
plt.plot(range(X.shape[1]), X[0, :, 0], 'k-')

for i in range(1, Y_pred.shape[2] + 1):
    plt.plot([x * 2 - i + 2 for x in range(i, Y_pred.shape[1] + i)],
             Y_pred[0, :, i-1],
             'r--', label=i, alpha=0.5)

plt.xlabel('time step', fontsize=14)
plt.ylabel('value', fontsize=14)
plt.title('Forecasting several times steps ahead', fontsize=18)
plt.show()
```


![png](homl_ch15_Processing-sequences-using-RNNs-and-CNNs_files/homl_ch15_Processing-sequences-using-RNNs-and-CNNs_56_0.png)


#### WaveNet

WaveNet is a convolutional neural network that has both a long and short term memory.
It is just a stack of convolutional layers where each successive layer has a doubled *dilation rate* (how spread apart each neuron's inputs are).
Thus, the lower layers with smaller dilation rates learn short-term patterns and the upper layers learn long term patterns.
The diagram for the architecture helps to understand this concept.

![](assets/ch15/wavenet_architecture.png)

For the example used in this notebook, WaveNet not only outperformed the RNNs, but it was also faster to run through the same number of epochs.


```python
wavenet_model = keras.models.Sequential()
wavenet_model.add(keras.layers.InputLayer(input_shape=[None, 1]))
for rate in (1, 2, 4, 8) * 2:
    wavenet_model.add(
        keras.layers.Conv1D(filters=20,
                            kernel_size=2,
                            padding='causal',
                            activation='relu',
                            dilation_rate=rate)
    )

wavenet_model.add(
    keras.layers.Conv1D(filters=10, kernel_size=1)
)

wavenet_model.compile(
    optimizer=keras.optimizers.Nadam(),
    loss=keras.losses.MeanSquaredError(),
    metrics=[last_time_step_mse]
)

history = wavenet_model.fit(
    X_train, Y_train,
    epochs=10,
    validation_data=(X_valid, Y_valid),
    verbose=0
)
```


```python
pd.DataFrame(history.history).plot(figsize=(8, 6))
plt.show()
```


![png](homl_ch15_Processing-sequences-using-RNNs-and-CNNs_files/homl_ch15_Processing-sequences-using-RNNs-and-CNNs_59_0.png)



```python
Y_pred = wavenet_model.predict(X_valid)
np.mean(keras.losses.mean_squared_error(Y_valid, Y_pred))
```




    0.022441423




```python
Y_pred = wavenet_model.predict(X_new)

fig = plt.figure(figsize=(8, 5))
plt.plot(range(X.shape[1]), X[0, :, 0], 'k-')

for i in range(1, Y_pred.shape[2] + 1):
    plt.plot(range(i, X_new.shape[1] + i),
             Y_pred[0, :, i-1],
             'r--', label=i, alpha=0.5)

plt.xlabel('time step', fontsize=14)
plt.ylabel('value', fontsize=14)
plt.title('Forecasting several times steps ahead', fontsize=18)
plt.show()
```


![png](homl_ch15_Processing-sequences-using-RNNs-and-CNNs_files/homl_ch15_Processing-sequences-using-RNNs-and-CNNs_61_0.png)



```python

```
