{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 7. Ensemble Learning and Random Forests\n",
    "\n",
    "Aggregating the predictions from multiple predictors is often better than an individual predictor (\"wisdom of the crowd\").\n",
    "These are called *ensemble methods*.\n",
    "It is common to use an ensemble method towards the end of a project after a few good individual classifiers have been found.\n",
    "\n",
    "This chapter discusses **bagging**, **boosting**, **stacking**, and **Random Forests**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "plt.style.use('seaborn-whitegrid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Voting classifiers\n",
    "\n",
    "The idea behind the power of ensemble method can be explained by thinking about a slightly biased coin. \n",
    "If the coin has a 51% chance of being heads, this won't be noticeable with a few coin flips, though, after thousands, the ratio of heads to tails will be distinct.\n",
    "This applies similarly to an ensemble of weak classifiers.\n",
    "However, one caveat is that the ensemble of weak classifiers are unlikely to be perfectly independent from each other as they are trained on the same data.\n",
    "Therefore, the strongest ensemble methods use multiple model types that use various training methodologies to create a diverse and more independent group of models.\n",
    "\n",
    "The following example shows the use of logisitic regression, a random forest classifier, and a SVM in a simple ensemble."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression: 0.8332\n",
      "RandomForestClassifier: 0.8628\n",
      "SVC: 0.8692\n",
      "VotingClassifier: 0.8632\n",
      "VotingClassifier: 0.8632\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.datasets import make_moons\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Individual classifiers.\n",
    "log_clf = LogisticRegression()\n",
    "rnd_clf = RandomForestClassifier(max_depth=3)\n",
    "svm_clf = SVC(probability=True)\n",
    "\n",
    "# Hard voting classifier ensemble.\n",
    "voting_clf_hard = VotingClassifier(\n",
    "    estimators=[('lr', log_clf), ('rf', rnd_clf), ('svc', svm_clf)],\n",
    "    voting='hard'\n",
    ")\n",
    "\n",
    "# Soft voting classifier ensemble.\n",
    "voting_clf_soft = VotingClassifier(\n",
    "    estimators=[('lr', log_clf), ('rf', rnd_clf), ('svc', svm_clf)],\n",
    "    voting='soft'\n",
    ")\n",
    "\n",
    "# Make moon data.\n",
    "X, y = make_moons(n_samples=10000, noise=0.4, random_state=0)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
    "\n",
    "# Train and test each model.\n",
    "for clf in (log_clf, rnd_clf, svm_clf, voting_clf_hard ,voting_clf_soft):\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    print(f'{clf.__class__.__name__}: {accuracy_score(y_test, y_pred)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If all of the models have a `predict_proba()` method, then the `VotingVlassifier` can use \"soft\" voting to weight each prediction by the certainty of the model (by just predicting the class with the highest average probability).\n",
    "This often produces better results than \"hard\" voting which predicts the most frequently predicted class over all of the models.\n",
    "\n",
    "## Bagging and pasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
